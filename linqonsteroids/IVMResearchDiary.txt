Warning: what follows is often close to a stream of consciousness!

2011 06 27 Monday:
- Read paper about ScalaQL from Spiewak und Zhao, various issues with it
- Discussion with Klaus: he proposed me to prototype incremental view maintenance for Scala, in a way which is integrated within Scala collections
- There might be a place for monadic language embeddings (like in language virtualization), where possible semantics rotate around execution and optimization.
- Alternatively, it might be worth to support a better syntax for Applicative-based language embeddings, given an important enough motivation.

2011 06 28 Tuesday:
- In the context of The Project, blurring the distinction between static analyses (happening at something like development/compile-time) and manipulations of e.g. expression trees (like in deep embeddings, LINQ, expression trees). Does that mean moving toward Lisp, again?
- Tillmann yesterday mentioned using incremental view maintenance for handling program modifications done by e.g. optimizations, to save time needed in recomputing static analyses. While the idea is interesting, it requires an extremely cool view maintenance algorithm: to update the results of static analysis, for instance it must be able to handle the fixpoint. Interestingly, there are algorithms for view maintenance able to handle incremental queries: some paper from Ooge de Moor discussed a translation of a Datalog subset using some kind of recursive relational queries, and the Datalog subset was indeed aimed at static analysis/code querying/refactoring: "CodeQuest: Scalable Source Code Queries with Datalog".
However, let's keep in mind that all incremental view maintenance algorithms are only heuristically more efficient than recomputing the whole view from scratch.
- Incremental update: what about changes to element?
  Sol. 1) Element are immutable, like keys in standard hashtables. But what about nested collections? If they are also immutable, then it makes sense for homogeneity that the 
  Updating an existing materialized view is in essence a side-effect, thus we cannot hope to get a purely functional solution.
  Sol. 2) Notify derived views about changes to single elements. The protocol for such updates is slightly problematic though. The simplest possibility is to encode element modification in term of addition and removal, but to encode that properly we need to signal the removal of the old value, followed by the insertion of the new value. That might be too onerous a constraint.
  As an alternative, one might notify the change of one element: in that case the listeners receive the identity of the changed object and of course its new value, but not the old one, complicating the update.
  This issue is present in general in incremental view maintenance - this is one of the choice in the modification dimension of the problem statement [2].

- Read some papers about incremental view maintenance. E.g.

  - [1] Ashish Gupta, Inderpal Singh Mumick, and V. S. Subrahmanian. 1993. Maintaining views incrementally. Proc. SIGMOD '93
  - [2] Ashish Gupta, Inderpal Singh Mumick, Maintenance of materialized views: problems, techniques, and applications, 714 cit., 1999 (survey paper, including probably the previous one)
http://portal.acm.org/citation.cfm?id=310737

2011 06 29 Wednesday:
- The Scala update protocol is cool, in that Observable report before and after the changes they get. In theory/according to Klaus. In practice that seems to not work - see the Update class:
https://lampsvn.epfl.ch/trac/scala/browser/scala/tags/R_2_9_0_1/src//library/scala/collection/script/Message.scala#L1.
Actually, as seen here:
https://lampsvn.epfl.ch/trac/scala/browser/scala/tags/R_2_9_0_1/src//library/scala/collection/mutable/ObservableBuffer.scala#L1
the standard protocol is to send an Undoable Update operation. But you can access the old element only by undoing the update, which will trigger another modification. The update message contains enough information to re-undo the update again. Anyway: we need a better ObservableBuffer at hand - quite a few operations seem to have no associated notification (say insert/insertAll); BufferLike docs require the implementation of insertAll in particular (insert is implemented in terms of it). One problem with the current protocol is that the Include message allows for just one element - which is suboptimal when you can optimize bulk operation. One could have a translator from a more complex protocol to a simpler one - a listener decorator - to allow a client to understand just a simple protocol, which doesn't directly support BulkInsert but supports it only indirectly in terms of its translation.
However REMEMBER, BACKWARDS COMPATIBILITY in research, is NOT a STRICT requirement.

- There are 2 ScalaQL around, and seemingly no implementation available of the work I studied. Ask Daniel Spiewak himself, and read the paper from the other group (which seem to be using however expression trees)! The second ScalaQL is:
http://www.sts.tu-harburg.de/people/mi.garcia/ScalaQL/ - the status is discussed here: http://scala-programming-language.1934581.n4.nabble.com/scala-tools-Status-of-ScalaQL-td1984651.html.

A page mentioning the first is the home of D. Spiewak: http://www.cs.uwm.edu/~dspiewak/
- Monadic queries are also possible in:
- https://github.com/szeiger/scala-query#readme
- See http://www.quora.com/ScalaQL

- Christian suggested looking for references about view maintenance in books shown here: http://wwwiti.cs.uni-magdeburg.de/iti_db/lehre/db2/2011/slides/teil1.pdf 
- Discussed with Klaus: we can derive primary-key constraints from Map instances.

2011 06 30 Thursday:

- Noticed this paper submission:
http://www.cs.utexas.edu/~wcook/Drafts/2011/batchdb.pdf
In this case, I think it's even clearer the relation to LINQ and to monads; the main difference is the operation reordering.

- IncViewMaint: implemented ObservableBuffer2, fixing a bug in ObservableBuffer, and a testcase showing notifications.
- we can use a smart update protocol when elements are observable themselves - including nested collections. Non-observable mutable elements are a problem, because we don't know when they are updated: either they are forbidden or the client is forced to manually send notifications for them.
- Some problems with incr. view maintenance, e.g. the need for maintaining counts of elements, are only present when using set semantics for relations (select distinct …), not when using duplicate semantics (which are more natural in most cases with collections). We should however also provide a "uniq" operator (if there isn't already one in Scala).
- Look at scala's SeqView - it already define a family of classes which represent modified views. We just need to extend all of them with more methods - (the expression problem appears again?).
  See in particular FlatMapped. We need in particular to have 
- We discussed the ability of optimizing queries in monadic form, by recognizing joins and possibly permutating the iteration order for operands of join. I think this would be another contribution; moreover it is not clear how to show safety of such reorderings if the reordered operations perform side effects. We have to see what the SAE/proto library does in this respect.
- TODO: Study the architecture of materialization/unmaterialization in Scala. It seems to be mediated mainly by the couple of methods view/force, which are surely inverse of each other for immutable objects. When identity is important new questions arise: is c.view.force the same object as c or a copy of it?

- Sequence operators on views produce in essence a data representation of a program to be interpreted to produce the view itself. When executing a complex query not within a database but by manipulation on views, interpretative overhead might arise. Since the query is known at compile time, we might instead convert it into a program in a polymorphically embedded DSL (produced by (inlined) interpretation of the query comprehension syntax), and provide different interpretations for view materialization, view update, and so on.
At this point, databases could become simple libraries - the only point in having a separate DB would be to share it between multiple programs. One might however instead communicate with such a database simply through an efficient variant of Remote Method Invocation, for instance Remote Batch Invocation or the like. Interoperability problems arise in this case - RBI as-is is somewhat interoperable; an alternative might be to implement all of this within a .NET environment with its better support for interoperability between languages. Staying within the JVM platform implies restricting support for such a protocol only to Scala - a standard protocol would have to be implemented for interoperability with other languages, leading to the question: Why do it? Alternatively, one could choose an interoperable RMI approach; but can such an approach be flexible enough? The collection interface that we need uses all features of Scala type-system.

2011 07 1 Friday
- whole day: preparation of guest Typechef lecture.
- Write down link to Jaque http://code.google.com/p/jaque/

2011 07 6 Wednesday:
- The original view content should be created as an update on the empty view, to reuse the code from notification.
Sunday 24 July:
- Scala Linq:
  - He uses FuncExp: (Exp(S) => Exp(T)) => Exp[S => T], i.e. HOAS. However, this form of HOAS assumes currying, which is NOT pervasively available in Scala. The correct uncurried version is:
FuncExp: (Exp(S1, S2, …, Sn) => Exp(T)) => Exp[(S1, S2, …, Sn) => T], with variations for n = 1, 2, …, until some implementation-limit.
- I didn’t discuss with Klaus the simple lifters (through implicit conversion) for function values! But his code seems to go in another direction…

  def testLiftedFuncs(l: MyList[String], j: MyList[String]) {
    import FunctionLifter._
    val r2 = for (k <- l; k2 <- j if (test _)(k,k2)) yield k+k2
    val r3 = for (k <- l; k2 <- j if (foo _)(k) is (bar _)(k2)) yield k+k2
  }
  //Experimental conversions.
  object FunctionLifter {
    // The implementations here are temporary, just to match the types.
    implicit def injectF[A, B](f: A => B): Exp[A] => Exp[B] = x => f(x.compile())
    implicit def injectF2[A, B, C](f: (A, B) => C): (Exp[A],Exp[B]) => Exp[C] = (x,y) => f(x.compile(), y.compile())
  }

Thursday Aug 4:
- Check paper on DryadLINQ
- TODO: prepare feature request for implicit search.
- How to write an implicit conversion which applies other implicits:
implicit toExp[T, S, That](t: T)(implicit toS: T => S, cbe: CanBuildExp[S, That]): That = 
cbe buildExp toGenericExp(toS(t))
The really cool thing here is that we get to apply two implicates, but only for an explicit design decision of one of the two libraries. A problem however might exist, due to type inference not working within one parameter list, but only from one to the other: this might be solved by having more implicit parameter lists.
If something like this really works, you might end up addressing one Haskell’s problem - but in the context of Scala. In Haskell, sometimes, you really need to lift many values explicitly to execute computations in a non-pure monad - or applicative functor. It would be much nicer if the need to lift such values could be inferred. Instead in Scala, thanks to implicit conversions, we have a pattern for implementing a library of such computations - and this is useful for language virtualization.
- What I just read: Scala implicit resolution can in some case choose between multiple methods the “most specific one”. It just does so by using only the argument type of the selector, rather than the target type.

Wednesday 24 August 2011
- HOAS -> FOAS conversion, as in Klaus idea, also appears within Conan Elliot’s Compiling embedded languages. That’s also related to place from “Boxes go bananas”.

- LinqOnSteroids: for tomorrow, my TODOS are in commit fa1dbe4807ff1575fc7120696bcf02b531620505:
- TODO: IncrementalResult needs also to propagate updates. (DONE)
- test flatMap!

Sun 28 Aug:
- I think that the equivalent to the iterator pattern are folds - that calls for IVM for folds.
- We probably want to have lazily initialized views, just as in the other framework. That would move the initialization cost for IncrementalResult to when it’s actually used.
- It would be cool to do incremental maintenance for folds by simply storing the intermediate results and doing the computation in a balanced tree - a change to any element would cause just a logarithmic number of recompilations. Splay trees come to mind: they’d allow doing even less recomputations when recently changed elements are changed again.

- Related work for IVM:
OOPSLA 2011, Two for the Price of One: A Model for Parallel and Incremental Computation

Wed 31:
- HOAS -> FOAS conversion sounds related to weak HOAS, but is not really - that’s Lam (Name -> Exp), not Lam (Exp -> Exp).

Mon 5 Sep:
- Convert Lifting.TraversableLifter.Join to split the casts away.
- Move IVM todos (like the one above) to a separate file.

IVM related works
MOVIE: An incremental maintenance system for materialized object views
Incremental Attribute Evaluation: A Flexible Algorithm for Lazy Update
The Monad.Reader 11 - How to Refold a Map (?)

New “bug” in Scala compiler: implicits are not applied for pattern matches.

IVM note: It is notable that in this new encoding, where Traversable[T] is no longer
"special", we could after all define withFilter as returning Exp[FilterMonadic[T, Repr]].
This encoding effort is starting however to require a bit too many classes - using FilterMonadic would avoid that! To get precise types, we’d then need to create an instance of TraversableOpsLike with the right params (as for Scala collections) and the associated conversions (instead of the associated builder object).

For IVM, I should build a test comparing, for each class, the results of interpretation and of incremental update. QuickCheck (and maybe ScalaCheck) can generate functions - that could be used for testing here?

19 Sep:
- what about Exp[T] <: Publisher[Message[T]], and having messages for a collection type T = Traversable[S] extend Message[Traversable[T]]? Sounds cool.
- Currently we can extend Exp with methods specific for our usage of expression trees. In the future, we’ll have to transform general expression trees in trees specific for us!

(First days of October)
==
Related work: http://ppl.stanford.edu/wiki/index.php/SLINQ

Virtualized pattern-matching:
Given:
case class Foo(a: Bar)
then:
exp match {
case Foo(a) =>b
}
should become:
Foo.unapply(exp) match {
case None => ()
case Some(a) => b
}
which should become:
val t /*$tmp *//*: Exp[Option[Bar]]*/ = Foo.unapply(exp)
if (!t.isDefined) 
  ()
else {
  val a = t.get
  b
}

This translation is/will be supported by expression trees, and it would make sense also when not using expression trees (since I avoided if (t is None)). I.e., that’s an untyped desugaring which is as flexible as Scala’s for comprehension. Can desugar without inspecting the type of unapply/unapplySeq?

== The difference between view.filter and withFilter ==
Traversable[T] defines view: TraversableView[T, Traversable[T]] and withFilter(…): FilterMonadic[T, Traversable[T]]. One can further call the same method on each of those two classes. They thus seem similar, but they are not, because TraversableView <: Traversable. Note the funny type that view must have on TraversableView:
def view: TraversableView[A, TraversableView[A, Coll]]
Instead, FilterMonadic >: Traversable, hence the returning object can have a more general type.
Indeed, withFilter returns FilterMonadic[T, Repr] when invoked on FilterMonadic[T, Repr], that is, e.g. on instances of Traversable.WithFilter[T, Repr] or of Traversable[T].

18 Oct 2011:
- Other closely related project to IVM:
http://code.google.com/p/scala-integrated-query/
Their embedding of Scala collections supports only Iterable collections. We do better.

19 Oct 2011:
the key trick is to register on children of a FlatMap only during interpretation, notification, or construction of IncrementalResults, but not when a FlatMap node is constructed itself, since FlatMap nodes are constructed also when Var is passed.

20 Oct 2011:
what if we get a list of tuples, but we want to treat it as a Map? We can use groupBy, yes; but what if we know that we have a unicity constraint on one column (or in general, the result of applying one function on each tuple) and want to take advantage of that to construct a simpler map than would be constructed by groupBy?
Traversable[T].groupBy(T => U): Map[U, Traversable[T]]; but sometimes we want just
Traversable[T].groupBy_(T => U): Map[U, T], or even Traversable[T].groupBy_(T => U, T => Rest): Map[U, Rest]. For that to make sense, we need an unicity constraint on the result!
- I duplicate the conversion from Coll[T] to Exp[Coll[T]] for each relevant collection.
Shouldn’t I have something like:
toCollExp[T, Repr](t: Exp[Repr])(implicit c: CanBuildFrom[T, Repr, Repr]), with sensible bounds like Repr <: Traversable[T] to help type inference? It would be nice, let’s try it soon.

23 Oct 2011:
- does the lifting code generator lifts methods which are available through implicit conversions? What should we do to handle that case?
Should be easy: given an implicit f: A => B, provide a corresponding implicit conversion of type (with quasi-quotes): ,(Exp[`A] => `Transformed(B)), where Transformed(B) is the lifted version of class B.
Given
implicit def f(a: A) = new B(a)
class B(a: A) {
def foo(params: Types) = …
}
we produce:
implicit def f_(a: Exp[A]) = new TransformedB(a)
class TransformedB(a: Exp[A]) {
def foo(params: Exp[Types]) = onExp(a, params)((x, params) => f(x).foo(params))
}
I.e., we include the implicit conversion in the lifted function, since we cannot invoke it on a: Exp[A] directly. Otherwise, we could have 2 onExp calls, and “call the first” eagerly before constructing TransformedB. Not that it really matters much, since no interpretation is going on. If the code reuses the result of an implicit conversion, we can ensure to reuse the expression tree node, which is a modest saving. Two onExp calls, however, produce an expression tree with a clearer structure.

24 Oct 2011:
# Our query language and data model compared to OO and semi-structured data
Our data model is not semi-structured; it is OO, but unlike pure OO systems, it’s also functional; in particular, pattern-matching is encouraged in Scala. It entails checking at runtime whether a value has a given type (instanceof) and deconstruction. They are both possible in classical OO systems, but instanceof is discouraged. Is it supported in classical OO database systems? Do they have type indexes as we do now, thanks to Klaus?

Handling of NULL values? Can we use Option for those values?

25 Oct 2011:
VLDB deadlines - papers submitted for the 1st December/1st January 2011 will get acceptance/rejection/revision requests within two months, and those requests will have to be handled within two months. The conference is in August 2012. So the deadline will be (unfortunately) 1st January.

26 Oct 2011:
- Map[K, V] <: Function1[K, Option[V]] or extends PartialFunction[K, V]? There is an isomorphism between the two types, expressed by the functions:

_.lift: PartialFunction[K, V] => Function[K, Option[V]]
Function.unlift: Function[K, Option[V]] => PartialFunction[K, V]

but using directly Function1[K, Option[V]] is faster when the value is needed (i.e. usually); converting Function1 to PartialFunction produces a slow PartialFunction, as noted in ScalaDocs for Function.unlift.
On instances of PartialFunction, one needs to invoke isDefinedAt to get a yes/no answer, and when yes, to invoke apply to get the actual value. Usually this implies doing part of the computation (e.g. pattern matching) within isDefinedAt; possibly all of it must be done, and then the result thrown away (see Function.unlift).

- People love column-store DBs - even if that’s just a storage model. Is that something we can easily support in our language? Is that something that our implementation language allows hiding?
“Database Architecture Evolution: Mammals Flourished long before Dinosaurs became Extinct” - http://www.vldb.org/pvldb/2/vldb09-10years.pdf
See the description of Radix-cluster - it becomes clear that we need to optimize also queries which take significant time after indexing. Btw, it’s also clear that we need to benchmark against e.g. SQLite in async mode, or something even faster.
But let’s remember that we have a few clear contributions - I shouldn’t build a full-scale database system in one go.

27 Oct 2011:
When is it better to recompute a materialized view from scratch than to update it?
Let’s consider the example query `res = c map f`, and the example update `c += x`. 
`res` is updated through `res += f(x)`. Such an update cannot remove elements from the result: we term such updates _expansive_. It is thus clear that recomputing the result from scratch implies recomputing all the elements which are already part of the result, and performing additional work for the new elements. The map operation is even “distributive”, because it’s a homomorphism on the monoid of lists. Incremental maintenance for expansive updates is always convenient.

For the same query, consider the example update `c -= x`. This time, `res` is updated through `res -= f(x)` if `f` is pure, since removal compares object for equality, not for identity. Otherwise, we would need to find the object originally produced when `f(x)` was added to `res` and remove it. We pay the cost of computing `f(x)` only to look it up and throw it away. If computing `f(x)` has a roughly constant cost for each input, when removing at least half of `c`, it is better to recompute `res` from scratch. We term such an incremental update _contractive_.

Consider now the query `res2 = c2 - res`. Here `res = c map f` and `c` appears with negative _polarity_ (this can be probably formalized also using data structure derivatives, where we’ll get a subtraction in front of `\Delta c`).
`c += x` will compute `f(x)` just to remove it from `res2`, indeed; however, recomputing that from scratch will not be helpful. Additionally, it makes much sense to materialize c map f here anyway to handle addition of elements to c2 - in general, set difference is not completely self-maintainable, unlike set union; the 2nd table must be materialized (the first needn’t).
Thus, in this case, we want decide the update strategy for `res` locally. It seems also convenient to just remove the new elements from `res2`.

Conclusion: for these example, it’s only removing elements which should sometimes be translated to a notification of a clear() followed by add(remaining elements). And that can always be decided locally, it seems. Given a set of adds and removes, it makes sense to perform the removes first (which might trigger recomputations) and then the adds (of course, if an element is first added and then removed, it should not be added in the end). IOW we have a directed semantic-preserving rewrite rule: `(a + b) - c => (a - c) + (b - c)`. Equational proof of correctness:
`(a + b) - c = (a + b) \land \neg c = a \land \neg c + b \land \neg c = (a - c) + (b - c)`.

Queries are contractive when:
- Usually when deleting elements
- Adding elements might contract the result when those elements are transformed into filters. E.g., in c withFilter f, imagine that f is x => c2 forall (x > _). forall is a map followed by a fold(false)(&&), and we don’t support folds yet in any case - but that’s an orthogonal problem, because we’ll indeed support `a - b` even now.

===

- The other idea for nested indexes is to represent them with multiindexes.
- Problem with first-class indexes: they were probably avoided for all this time because they seem to tie queries with implementation details. What happens when an index is replaced by a non-materialized view? We might then need an optimization creating the index on the fly when worth, to make queries using a non-materialized index more efficient. Otherwise, explicit index application will only produce a query which is fragile wrt index changes. Standard optimizations to get some indexes applied are anyway also fragile wrt the same changes.

- I plan to implement functional updates on query results (e.g., `res + c` producing a new incrementally maintained query). Actually, that’s kind of already supported because IncrementalResult[T] <: Exp[Set[T]].

==
Rewriting IncrementalResult.
orig.toMaintainable could be implemented as c.withFilter (_._2 > 0).map (_._1) on top of a “counting map c”. The counting map can then be expressed as orig.map((_, 1)).groupBy(_._1).map((k, v) => (k, v.sum))
Let’s pack this altogether:
orig.map((_, 1)).groupBy(_._1).map((k, v) => (k, v.fold(0)(+)).withFilter (_._2 > 0).map (_._1)

Using groupBy and a fold on a monoid is a very compact way to express incremental updates on maps - however, we need them exactly to implement maintenance of groupBy results, and thus of indexes. Hence we need a common different basic operator - groupByAndFold. The important thing is that while sum here is a fold, it can be maintained. But that’s orthogonal.

Here, the point would be to materialize the first part, possibly up to and including the final filter:
orig.map((_, 1)).groupByAndFold(_._1, 0, _+_).withFilter (_._2 > 0).force.view.map (_._1)

groupBy(f)(bf: CanBuildFrom) = groupByAndFold(f, bf(), _ ++ List(b), _ --  List(b)) /*Small lists are smaller than small Traversable = small Vectors; +, :+, +: are not defined on Traversable.*/

IncMaint for:
GroupByAndFold[A, B, K](toKey: A => K, z: B, op: (B, A) => B, invop: (B, A) => B) extends Exp[Map[K, B]] {
//Note: this class is not self-maintainable without itself.
def transformedMessages(msg) = msg match {
case Include(el) =>
val key = toKey(el)
//this(key) = op(this.getOrElse(key, z), el)
UpdateMap(key, op(this.getOrElse(key, z), el))
case Remove(el) =>
val key = toKey(el)
//this(key) = invop(this(key), el) //Precondition: el \in this.
UpdateMap(key, invop(this(key), el))
}
}
This avoids the need to have special messages Message[Map[K, V]].

NOTE: set union and maps are so cool that they are incrementally maintainable without even reference to themselves. This concept is not described in the literature I know of about IVM.

== Many of our operations (joins, indexes, and so on) allow arbitrary functions where standard DB theory doesn’t. Is there a tradeoff?
Yep. What’s the cost of those operations? How do I optimize when considering general functions? Canonicalize the representation: resulting tree sound easier to manage. OTOH, indexes on OO databases use generic attributes, and thus face potentially a similar problem - don’t they?

Consider the example of Join:
Join(colouter: Exp[Traversable[T]], colinner: Exp[Traversable[S]], outerKeySelector: FuncExp[T, TKey], innerKeySelector: FuncExp[S, TKey], resultSelector: FuncExp[(T, S), TResult])

==
- Set intersection:
A - (A - B) =
A \intersect \neg (A \intersect \neg B) = A \intersect (\neg A \union B) = A \intersect \neg A \union A \intersect B =
A \intersect B

==
A relation can be viewed as a map. In DB theory, there is the concept of nested relations, which sounds clearly related.
Google Scholar “nested relations” - e.g. A recursive algebra for nested relations

==
How can we express outer joins in our setting?
Inner join:
coll1 flatMap(x => coll2.withFilter(y => f(x) == g(y)).map(y => h(x, y)))
Which could maybe become sth. like:
coll1 flatMap (x => coll2.groupBy(g).get(f(x)).flatMap(y => h(x, y))) //Can’t be transformed yet into a join operation. Note the use of get: => Option[V] instead of apply: => V

This could be executed somewhat efficiently by unnesting and making sure to materialize b.groupBy(g). We’d have still a full table scan for the outer table, though.
We should allow expressing *intersection* of dictionaries on their keys within the query language, rather than just through Join nodes - which are really too complex.

Left(?) outer join:
coll1 flatMap (x => coll2.withFilter(y => f(x) == g(y)).map(Some(_)).addIfEmpty(None).map(y => h(x, y))) which becomes, after indexing:
a flatMap (x => b.groupBy(g).get(f(x)).map(_.map(Some(_))).getOrElse(Traversable(None)).map(y => h(x, y)))
This is complicated to express for two reasons:
- Left joins make little sense and rely on NULL
- None has a distinct type from object values; we must convert values to a nullable column
- we could always hide this into a separate pimpled method:
x: Option[Traversable[T]] => x.map(_.map(Some(_))).getOrElse(Traversable(None))

Shall we say that we don’t want to express outer joins for now? Yep.

More interestingly: how do we express the body of an inner join so that it can be optimized/incrementally maintained as well? We want also IVM to reuse the ideas from hash-join!
==
30 Oct 2011:
Is Exp an applicative functor? Yep, because it embeds lambda-calculus primitives HOAS, which are well known (I’d expect) to include the applicative functor interface (minus pure, which we do have anyway). Implement that with Scalaz.

<*> = apply: Exp[A => B] => Exp[A] => Exp[B] (producing App nodes)
pure = toExp (producing Const)

HOAS representation adds lam, for us FuncExp: (Exp[S] => Exp[T]) => Exp[S => T], the inverse of App.

==

18 Nov 2011:
Hints on Programming Language Design, Hoare, 1973. He makes the exact same points as we make for query optimization: for transparency, it should be a 
The point is less valid for programming languages nowadays due to technology advances; however, DB optimizations are concerned with order-of-magnitude speedups, thus optimization transparency there becomes all the more important.

21 Nov 2011:

= Why it is hard to support Exp[Seq] explicitly =
I thought that IVM on Seq was harder to support. Indeed, there are more details to manage (think especially of flatMap); however, that's not the most complex problem.
As pointed out by [*], different implementations of joins are equivalent only if the result is regarded as a set, not as a list. For instance, a join executed through nested loops returns results in different orders depending on which collection is the outer one.
[*] Sec. 6.6 of "Generic Multiset Programming for Language-integrated Querying":
"The problem is that the cross-product then needs to define a particular order for its pairs that each implementation must abide by, but the order is different depending on which join algorithm is used: nested iteration, sort-merge join, hash join or discriminatory join."
So, the solution is first given by T. Grust (2004), "XQuery on SQL hosts".

However, Klaus explained that joins can be order-preserving if needed.

= Relational algebra vs relational calculus =
Relational algebra uses explicit select, project etc. steps; the two relational calculi (tuple r.c. and domain r.c.) instead are in essence restricted variants of set comprehensions, where variables range over either tuples or domains.

24 Nov 2011:
Discussion with Sebastian about folds
I imagined that binary operators can be defined with type: (InvExp[T], InvExp[T] => InvExp[T]), and the resulting expression tree could be automatically inverted. However this interface does not forbid things like a + a. While this is easily invertible for +, it is not invertible for a generic binary function given its inverse in one direction. So one does have to pass to fold invertible functions.

Tillmann did mention partial isomorphisms, which are however partial; there were before non-partial variants, but they're unary, not binary.
So maybe I have to go back to something like Sebastian's invertible combinators - but I don't believe they can really made easily usable.
Haskell allows to write things like:
f :: ::  Num b => b -> b -> b -> b
f = ((+) .) . (+) 
but they are indeed hard to read.
Sebastian wrote as an example: Comp(f, g)(o, i) = g(f(o, i), i), and that's indeed the only composition with asymmetric types if we don't consider right-folds. But of course, with symmetric functions, there are n permutations of the above.

[Type inference for this term: 
(+) :: Num a => a -> a -> a
(.) :: (b -> c) -> (a -> b) -> (a -> c)
((+) .) :: Num b => (a -> b) -> (a -> b -> b)


((+) .) . (+) :: [(.) :: (b -> c) -> (a -> b) -> (a -> c)] applied, with [b |-> Num b1 => (a1 -> b1), c |-> (a1 -> b1 -> b1), a |-> Num a2 => a2, b |-> Num a2 => a2 -> a2, which gives a1 |-> b1, a2 |-> b1, b |-> Num b1 => b1 -> b1, a |-> Num b1 => b1, c |-> Num b1 => b1 -> b1 -> b1] so we get (a -> c) |-> Num b1 -> b1 -> b1 -> b1
]

25 Nov 2011:
- How do we allow to globally optimize the IVM plan?
IVM updates needs to send whole collections, as we know; so send AddSeq: Seq[T] => Message[Traversable[T]] instead of Add: T => Message[Traversable[T]]. They might additionally receive queries, to enable later operators to use different update strategies. How do we share computations? The used expression nodes should cache the result of interpret(), in case multiple derived collections are active (XXX we didn't handle GC yet). How do we invalidate the cache?
We could use a global version counter, incremented by the original update. Moreover, updates performed during IVM might invalidate the caches… but actually, if we don't track those dependencies, results will indeed be invalidated when updating one derived collection and before updating the other. Instead of invalidating the cache, we need to incrementally update it when it's used! This means that the query tree should have materializing expression nodes.
In sum: IVM performs updates which might cause other updates. We need to construct examples where this happens or prove the lack thereof.

A much simpler solution seems to iterate over listeners of a collection we are updating, and do that recursively.

- First-class support for Bags!
1) The result of aggregation operators depends on multiplicities (see database books, like Garcia-Molina et al.).
2) We don't need to unify duplicated elements
3) We still ignore the order
4) If Bag <: Map[V, Int] <: Iterable[(V, Int)], an IncrementalResultSet could be implemented as Bag.map(_._1).

- Potential advantage of discriminator-based equality for Java?
Henglein explains that Java hash codes must be stored on object - that's true when objects are used and then moved (IIRC, MicroJVMs at least don't always store the hashcode, but check). However, it's not clear to me how he avoids requiring stable hashcodes: I need to study the exact semantics of page 58 I guess.
Anyway, if this would be reproducible in Java (which I guess it isn't), one could avoid the hashcode overhead.
ftp://ftp.diku.dk/diku/semantics/papers/D-608.pdf

Another point is that sort-based discrimination is linear-time while sort-based.
More importantly, let's consider their sdisc for bags: instead of hashing bags, they are sorted and lexicographically compared.
Now, according to this paper, sort-join will soon be faster than hash-join. Maybe discriminator-based join is already faster? The contribution here would just be testing of this hypothesis and validation, but that'd be cool, isn't it?
[1] Sort vs. Hash Revisited: Fast Join Implementation on Modern Multi-Core CPUs
Sometimes discriminator-join is two-pass, and is not yet parallelized (it's not divide-and-conquer), but it has probably more cache locality than hash-join (actually, partitioned hash-join already handles concerns about cache locality).

That paper also mentions that databases operating in-memory will become more common in the near future.

Henglein 2010, PEPM: he writes "select p1 (select p2 s)" claiming that deforestation in Haskell isn't needed, but that's not true (at least, not for the reason and citation he argues - doesn't he know on the work on deforestation in Haskell?).

An important motivation for using discrimination is that it does not require equals() and hashCode() to be kept in sync.

==
Related work:
Questions about "Reactive Imperative Programming with Dataflow Constraints"
- is the programming model reasonably natural, as argued?
- how does it compare to self-adjusting computation?
- is the slowdown acceptable?

Patrick Lamand, Eric Bodden et al. cite this this paper and argues that incremental computation could speed-up the Soot implementation in a workshop paper (The {Soot} framework for {Java} program analysis: a retrospective).

http://www.bodden.de/pubs/lblh11soot.pdf

"Demand-driven evaluation of collection attributes", cited in "Building semantic editors using JastAdd: tool demonstration" (http://dl.acm.org/citation.cfm?id=1988794), also seems related work (as are Attribute Grammars, AGs, in general). However, incremental evaluation of RAGs (Reference AGs) is still ongoing work (as opposed to AGs, I guess).

6 Dec 2011:
From "Caching and Incrementalization in the Java language" (Darren Willis, David J. Pearce and James Noble, OOPSLA 2008):
"An important issue in this respect is the query/update ratio for a particular query. This arises because there is a cost associated with incrementally maintaining a cached result set: when the number of updates affecting a result set is high, compared with how often it is actually used, it becomes uneconomical to cache that result set. To deal with this, our system monitors the query/update ratio and dynamically determines when to begin, and when to stop caching the result set."
We instead currently leave this issue to the user.

About the idea of sending query trees as update messages: the problem with that would be "how to avoid recomputing stuff when executing things multiple times".
Now, if cells cache their results, or at least all complete trees do so, we can just have interpret() (or result()) reuse the cache.

7 Dec 2011:

We discussed with Darmstadt guys optimizations based on unnesting (also based on "Optimizing Object Queries Using an Effective Calculus") and an evaluation based on FindBugs-like queries.
Tillmann explained me that atomic blocks (like the one in Imperative reactive programming with dataflow constraints) are non-modular, as he learned from (I guess) this thread from Haskell-cafe and Haskell-libraries:

http://www.haskell.org/pipermail/libraries/2010-March/013310.html
http://www.haskell.org/pipermail/libraries/2010-April/013420.html

Now, the idea we seemed to agree on was that adding scoping to constraint disabling, so that each module could disable its own constraints without disabling another modules constraints, should solve the problem. This has possibly impact on our definition of update batching.

9 Dec 2011:
We need to recognize constant functions, so that their results can be incrementally maintained - re-executing the query would create a new instance of it. This should maybe be changed using hash-consing as in TypeChef boolean formulas.

14 Dec 2011:
One important point for IVM in programs, and for doing that based on the observer pattern, is that it also enables to use notifications for performing external side effects.

25 Dec 2011:
I read again "Normalization by evaluation with typed abstract syntax". In their work, no first-order abstract syntax is used, and both the meta-level and the object-level use the ASTs they introduce. In particular, they mention introduction of FOAS and do not discuss converting FOAS back to HOAS - which is what we cannot do efficiently.

We still need to stop using CanBuildFrom for Views or to add extra instances to fix the problems we have. We produce terms of type Exp[TraversableView[T, Traversable[_]]].

30 Dec 2011:
Paper topics: What do we embed:

- embedding the lambda-calculus; lifting values (cross-stage persistence through interpretation); performance for HOAS/FOAS conversion
- tuples; embedding functions without currying; mention embedding of base types.
- Embedding of Traversable, Seq, Set, Map and Option; we embed the whole hierarchy, unlike ScalaIntegratedQuery (see their thesis); we reuse CanBuildFrom.
In general, we can define a functor for lifting existing interfaces.
- discuss operations on collections compared at least to relational algebra (or some more appropriate conceptual framework).
- issues with implicit conversions: combining two implicit conversions explicitly, existing implicit conversions must be manually lifted because:
a) implicit def foo: A => B won't be applied before converting B => Exp[B]
b) foo, as above, will not be applied to Exp[A].
- disabling lifting for Unit (and mutable collections)
- withFilter and Views - why Views should not use CanBuildFrom
- pattern-matching problems; effects; use of Let

Optimization:
- optimizing function bodies
- recognizing function equality for opaque functions (Call* nodes, analogue to function lifted through cross-stage persistence)
- function equality for expression trees
- examples of well-known optimizations - fusion, join recognition, unnesting (?) 

We require expressions to be side-effect-free - is this a significant limitation?

2 Jan 2012:
- I needed to use Let while building an expression tree. The current way to simulate this is:
  //The use of _App_ and FuncExp means that t will be evaluated only once.
  def letExp[T, U](t: Exp[T])(f: Exp[T] => Exp[U]): Exp[U] = App(FuncExp(f), t)

This must be used within buildAntiJoin - it can be avoided by defining a specific AntiJoin node.

Introducing such a node would mean that we actually need to implement beta-reduction before comparison.

Write in this paper, instead, that we do not create redexes, and that Lets we need for optimization are within interpret().

Moreover, nested Lets don't even work, because I implemented static scoping through dynamic scoping; all interpreters using static scoping must pass the environment through recursive calls to interpret.

- I realized how reassociation is possible: when reassociating terms with identical commutative and associative operations, always move constants on the left side, and nest them deeper; i.e., given a + (x + b), with a, b constants, always produce (a + b) + x. Look this up in the compiler book. (DONE)

12 Jan 2012:
- together with Tillmann, fixed the problem with the encoding of Option.
- questions to ask to Klaus:
  - can we use indexing as an optimization
  - which optimizations should we include?
  - which result should we aim for?

13 Jan 2012:
Tips from Christian:
- write the paper first as a solution plan (with "we will implement"), without stopping for coding
- we need, for each optimization we present, to show some query which gets faster
- we needn't be fast on all queries
- consider algebraic optimizations from text books, show stupid query which gets faster, and motivate the result by modularity
- handcode queries to check their speed
- implement "moving filter as early as possible"
- collaboration with him is OK
What I implemented after that:
- implemented hoistFilter (which moves filter as early as possible).
- implemented removal of stupid Let
- implementing these optimizations was much more complex, and taught me that it's still essentially untyped what we do, even when enabling some typechecking: the new thing are Exp.transform() calls which produce an expression with a different type.

14 Jan 2012:
- for the DB paper, I should also implement pretty-printing of expressions to something more similar to the source language, somehow. Pretty hard for Call nodes as they are!
Note: that comes almost for free (or with hopefully small changes) when using Delite.
Moreover, that would be useful during development too, since mentally converting from one style of writing expressions to the other takes time.
Pretty-printing is needed for the DB paper since we talk about "in-language optimization" and the ability to inspect the result of optimization - we could argue for that even without pretty-printing, but it would be a bit harder.

- Something which I forgot, about composability of implicit conversions (here called view):
"The problem is that views don't chain. Instead, use view bounds. In this [A <% Seq] notation, we accept any type A that can be implicitly converted to a Seq."
From [Scala complexity] http://yz.mit.edu/wp/true-scala-complexity/. However, commit 8ae00b2a444de091329600d13f79cacd9200e90e shows that "view bounds" don't work.

Again from there: this code:
new {
   def runs(f: (A,A) => Boolean) = 0
   def isMajority(x: A) = false
}
uses refinement types and reflection, which is bad and slow!
- I'm wondering: should I also write a paper about how type inference in Scala sucks and how it makes life difficult for the programmer, because it is unpredictable?
I'd need to get comments on a draft of this, and get suggestions about how to proceed.
16 Jan:
- would it be possible to inject implicit definitions as constraints for constraint-based type inference in Scala? See various/ImplicitParamBugReport.scala; it seems that there implicits are not helping type inference, but only depending on some strange circumstances. 

17 Jan:
- After meeting with Klaus, results:
  - target OOPSLA, not GPCE
  - also report on "lessions learned" on the difficulties caused by Scala
  - it makes sense to try indexing
  - run queries with FindBugs to compare runtime
  - address the concern that our code uses a different library (BAT instead of BCEL), and try separating the resolution phase from the actual querying phase.
  - transitive closure: introduce it as optimization (if we need it at all), and use the algorithm in FindBugs to achieve comparable performance.
  - modularity of queries thanks to our optimizations
  - his suggestion: "understand why the overhead is so high"; my suggestion: yes, but first concentrate on order-of-magnitude speedups.
  - concentrating on the evaluation and on what is relevant for that.
  - Klaus proposed selecting FindBugs queries interesting for us and hand-coding in a few cases the compiled query, to show what we would get if we had compilation.
  - Ignore OptiQL for now but keep it in mind, instead of contacting them and risk waking them up.

26 Jan:
- Boolean operators are not symmetric because they are not short-circuiting!
- Talking with Klaus, he suggested that we shouldn't offer short-circuiting operators
  - However, I'm still supporting them for now, and I improved their support; support for them is an implementation detail though.
- Why does else_ if_ work? Because else_ is a method and if_ is directly available. Hence it follows that parsing in Scala uses much more of the lexer hack than C.
Apparently, however, that's not necessarily the case - parsing seems to proceed as-if methods were there, and then they're later looked up. This means however that the AST is significantly restructured (to accommodate precedence) after parsing proper.
- Links about transitive closure implementations: http://www.cs.hut.fi/~enu/tc.html
http://scholar.google.com/scholar?q=transitive+closure+efficient+algorithm&hl=en&as_sdt=0&as_vis=1&oi=scholart

27 Jan:
Paper topics, v2: 

*) What do we embed:

- embedding the lambda-calculus; lifting values (cross-stage persistence through interpretation); performance for HOAS/FOAS conversion
- tuples; embedding functions without currying; mention embedding of base types.
- Embedding of Traversable, Seq, Set, Map and Option; we embed the whole hierarchy, unlike ScalaIntegratedQuery (see their thesis); we reuse CanBuildFrom.
In general, we can define a functor for lifting existing interfaces.
- discuss operations on collections compared at least to relational algebra (or some more appropriate conceptual framework).
- issues with implicit conversions: combining two implicit conversions explicitly, existing implicit conversions must be manually lifted because:
a) implicit def foo: A => B won't be applied before converting B => Exp[B]
b) foo, as above, will not be applied to Exp[A].
- disabling lifting for Unit (and mutable collections)
- withFilter and Views - why Views should not use CanBuildFrom
- pattern-matching problems; effects; use of Let

*) 
*) Optimizations support writing queries modularly and compositionally, without optimizing them by hand (because optimization is not a modular activity)
*) Evaluation: we reduce the size of code one writes
