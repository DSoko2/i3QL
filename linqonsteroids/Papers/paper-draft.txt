% Optimized queries over in-memory collections
% Paolo G. Giarrusso, Klaus Ostermann
<!-- % An embedded language for optimizable incremental queries over in-memory collections -->
\newcommand{\intersect}{\wedge}

# Meta-introduction
This project, variously called LINQ on Steroids, IVM, and part of The Project (™), implements a deeply embedded DSL for queries over collections and Incremental View Maintenance (IVM) for views defined using this query language.
It is related to projects presented at ECOOP and OOPSLA from students of James Noble.

A possibility is to publish this work without support for Incremental View Maintenance. A possible introduction would be as follows.

# Introduction
Many computations in typical programs extract information from collections. Higher-order operators allow formulating many such computations as queries; a collection library provides in other words a DSEL of queries over collections, which we call the query DSEL.

Research in query processing in database has identified many optimizations which afford great speedups, from high-level restructuring to index application and incremental view maintenance, yet the typical program does not take advantage of them for processing over in-memory collection, because performing these optimizations by hand leads to a complex, error-prone and hard-to-maintain restructuring of the original program. Scala offers operators like `map`, `flatMap`, `withFilter` and `union`, which are present in different form in many functional languages, together with for comprehensions, similar to list comprehensions in other languages: they are a form of syntactic sugar for accessing `map`, `flatMap` and `withFilter`. We consider these operators to form a DSEL for queries over collection. We reimplement this DSEL to allow optimization of queries, we implement a few significant ones, and show that they provide a significant speedup with an evaluation on (to be determined). We first design and implement a deep embedding of a portion of this DSEL. On top of this deep embedding, we implement some simple optimizations as a first step.

One of the design goals for this DSEL is that embedded queries should be as close as possible to their native equivalent, to simplify transforming a native query into an embedded one. Ideally, querying a "smart" collection instead of a native one should be enough to ensure to produce a representation of the query instead of executing it; however, a query representation needs to be then converted explicitly to its result. However, Scala does not support natively expression trees, which are used in C# to achieve this goal. We instead provide an emulation for them, based on user-defined implicit conversions, which we later describe. The surface syntax is quite close to Scala in many regards, but some areas show significant limitations; in other cases, providing a usable surface syntax is made more complex by limitations of Scala's type inference; one of our goals is to not require the user to supply more type annotations than it would need if expression trees were natively supported.

%Another possibility would be to publish this work in a more complete form. The rest of this paper is a draft which follows this idea.
% The above is not really true anymore.
# Introduction
Many computations in typical programs extract information from collections. Higher-order operators allow formulating many such computations as queries; a collection library provides in other words a DSEL of queries over collections, which we call the query DSEL.

It is often desirable to optimize execution of queries using a variety of approaches. For instance, some queries are often re-executed after changes on the original collections; other queries might be executed faster by maintaining additional data structures, like indexes over the original collections; yet other queries might be executed faster simply by restructuring the query before execution.
To implement these optimizations, we embed the query DSEL _deeply_, so that our library constructs in-memory representation of queries in form of abstract syntax trees before executing them. These ASTs can then be subject to optimizations before being executed on in-memory collections.
We propose a deeply-embedded Scala DSEL for expressing queries over in-memory collections with the following desirable properties:

- the query language is based on a hierarchical data model and the associated monadic query language, like the one used for Scala for comprehension, and similarly to the one described by Meijer and Biederman (2011) and Fegaras and Maier (2000)
- queries are expressed using for comprehensions
- queries can be interpreted with a very small overhead compared to queries expressed directly using Scala’s collection library
- queries can have, as results, both Traversable, Set and Map (support for Seq should be possible, too)
- our embedding techniques (we are embedding monadic operations over queries) extends the techniques used in the Scala collection library ([Fighting Bitrot with Types]) and allows, without code duplication, to have very specific types for the result of different operations; `flatMap` on a Map will return a Map, and on a Set will return a Set, and so on.
- results of queries over mutable collections are incrementally updated when the original collection changes.
- indexes can be generated and applied - they are simply queries of type Map[A, B], which are updated thanks to same support for IVM as other query results. This is only possible because our data representation allows to also represent different kinds of collections uniformly.
- queries can be optimized before execution; the optimized queries, unlike in SQL, can still be expressed in the same language
- we embed a significant subset of pure Scala, and identify problems which prevent using (?)
- the complete pure subset of Scala can be used in the queries (? - elaborate), except function calls (…) and pattern matching and …
- we have a partial emulation of expression trees in Scala through implicit conversions, and suggest approaches to extend the covered Scala subset.

Incremental maintenance of the results of `flatMap` is a novel algorithm.

Our contributions are as follows:

- index are identified with materialized views representing dictionaries, and can be used explicitly; this simplifies query optimization;
- embedded queries have precise types, just as the native Scala collections; in particular, our embedding supports both Traversable, Set and Map;
- the architecture used to implement IVM on expression trees is novel;
- we support Incremental View Maintainance for `flatMap` operations.

The rest of this paper is organized as follows. Sec. X describes the language we support; Sec. X describe how we embed this language; Sec. X discusses optimizations; Sec. X evaluates our framework by reimplementing queries from FindBugs; Sec. X discusses related work and Sec. X discusses future work and concludes.

# Our query language: first-class indexes considered helpful
Query languages for relational and OO databases allow to program queries declaratively, and optimizers transform those queries into execution plans for efficient execution.
Optimizers however often fail optimizing a query to an efficient execution plan: In this case, developers modify their queries to a different form until the results of optimization are the expected ones.
Disadvantages are:
- Understanding the output of optimization requires understanding a different language than SQL.
- The optimizer has access to ‘built-in functions’ which the programmer cannot use.
- Producing the desired execution plan might in general be impossible. Since the result of optimization is expressed in a quite different form, it is impossible to write directly an execution plan.
The reason for this is that execution plans contain many low-level details, with which the developer should have nothing to do.

We maintain that as many optimizations as possible should produce a program in the same language, that is, it should be _closed_. To this end, we introduce a query language embedded in Scala based on for-comprehensions, where indexes are first-class entities like tables. Transformation of queries done to benefit from indexing becomes then a closed transformation, which can be done either automatically or manually.
We do not maintain that *all* optimizations should be accessible to the user. However, it should be possible to at least perform transformations which change the time complexity of queries.
That is why in our proposal, indexes are made part of the logical level and become therefore accessible.

Our prototype implementation supports only in-memory collections, but this already allows showing the potential of the language.

<!-- Problem: what happens for e.g. join order? We usually want to keep that to be decided by the database -->
This situation resembles logic programming: a declarative approach is proposed, but efficient execution demands manual optimizations which break the elegance of the original model.
Functional programming languages, on the other side, allow expressing the intent in a clear way which also lends itself to reasonably efficient execution (compared to logic programming) and to further optimizations.

<!-- Is this regularity? -->

# Representing indexes: The groupBy operator
Indexes do not require special support; they are encoded through the `groupBy` operator. A query of the form:

    coll filter (x => f(x) == y)

is semantically equivalent to a query which builds an index first with the result of applying `f` to elements of `coll`, and then selecting the entry for `y`:
<!--can be executed as a lookup on an query building an index first:-->

    coll groupBy f apply y

What the optimizer does is (a variant of common subexpression elimination) between the resulting query and the keys of a map existing precomputed expressions.
A repository contains a map from queries to their evaluated results (XXX: it should be incrementally maintained queries!).

XXX: show a query that we can help using indexing.

## Indexes for nested objects
Queries often traverse multiple objects, and more complex index organizations are needed. \citet{Bertino89} describe three index organizations for this scenario, together with the corresponding optimizations to use them: nested indexes, path indexes and multiindexes. Nested indexes require reverse references between objects to be maintained, but since we do not maintain reverse references, we exclude nested indexes from further discussion.
We can readily use multiple groupBy indexes together to encode multiindexes \citep{Bertino89}; our framework allows defining also path indexes through specific operators.

# A few running examples:

Example 1 - a join expressed through a set comprehension, as a filtered Cartesian product (expressed through `flatMap`):

~~~
for (i <- coll1;
      j <- coll2;
      if i.field1 === j.field2)
  yield (i, j)
~~~

Example 2 - a Cartesian product query which cannot be transformed into a join - it could be manually rewritten though:

~~~
for (i <- coll1;
      j <- coll2;
      if i.foo + j.bar === 3)
  yield (i, j)
~~~

Example 3 - a query which could not be expressed as a Cartesian product:

~~~
for (i <- coll1;
      j <- i.divisors;
      if i.foo + j.bar === 3)
  yield (i, j)
~~~

# Operators
## Higher-order operators from relational algebra
- Join: can be expressed through `flatMap`, which can be executed directly (similarly to a nested-loop join) or converted to a specific `join` node.
- Selection: becomes `withFilter`.
- Projection: becomes `map`.
- Rename: since it makes less sense as such, it becomes `map` as well.
- Aggregation: becomes `fold`.

## Set basic operators from logic and relational algebra
- Set union: cannot be expressed through other operators
- Set difference can be expressed in the base language through withFilter and forall (in time O($N^2$)), but also with a specific, direct operator, which can be executed similarly to an anti-join.
- Set intersection: can be expressed through set difference ($A \intersect B = A - (A - B)$) or through a join.
but probably should be expressed directly, not through other operators.

Set intersection and set difference allow unseating collections built within withFilter operations.

It could be useful to have a let operation to support directly hoisting out of loops.

## Other kinds of joins
Semi-joins, anti-joins.

## IVM properties of those operators
Set union, selection and mapping are completely self-maintainable: that is, not even the result is needed for self-maintenance.
Tables used in filters, instead, must be materialized to allow self-maintenance; the result, again, is not needed.
`flatMap` is more complex. Results of `flatMap` require instead to store part of the results.

# Expression trees and nodes
We reify expression as expression trees. Each term of type `T` is represented by a node `n` with type `Exp[T]`; `n.subterms` returns a list of subterms, each of type `Exp[S] forSome {type S}`, but for different `S`. The simplest node is `case class Const[T](t: T) extends Exp[T]`. Note that `Const` nodes have no subterms - they just contain an evaluated expression.

Lambda abstractions have type `Exp[S => T]`. Values of function type (of type `S => T`) can be lifted through `Const`; in addition, we support higher-order abstract syntax (HOAS) and thus embed a term constructor `FuncExp` (classically called `Lam`), of type `(Exp[S] => Exp[T]) => Exp[S => T]`; however, we allow converting these terms to first-order abstract syntax (FOAS) through a standard trick; they have a single subterm, their body, of type `Exp[T]`, which is an open term containing also terms of type `TypedVar[S]`. This
This conversion is always done for three reasons:

- it removes the interpretative overhead due, e.g., to function composition and so on (see also higher), through _normalization by evaluation_ (XXX add citations). See also "Building blocks for performance oriented DSLs", where they do the same kind of trick to remove overhead from higher-order functions, even though they do not cite normalization by evaluation. [XXX note: it is not appropriate to cite Danvy's paper here, since _their_ normalization by evaluation is not what we need.].
- it catches early exotic terms, which call interpret() - `interpret()` will be called on open terms and throw an exception when recursing on TypedVar[S] terms.
%(there's a citation for that in Tillmann's TSR paper or in the "Boxes go bananas" paper)

- a FOAS representation allows performing optimization on function bodies.

What is not possible efficiently, to the best of our knowledge, is to convert FOAS back to HOAS. Given `e: Exp[T]` containing `v: TypedVar[S]`, the body can be expressed as `(x: Exp[S]) => e.substVar(v, x)`, which contains a substitution at runtime.

Note, moreover, that while the interface of our representation is well-typed and type-safe, its internals are well-typed but not automatically type-safe; in other words, the compiler does not guarantee that our code is safe, because in the implementation we rely on erasure and employ casts. We thus need to prove separately that the implementation is type-safe. However, this is common to most other implementations of expression trees, which rely on an untyped core tree wrapped through a phantom type.

## Evaluation
Closed terms %or "Expression trees representing closed terms"? I need to ensure, before in the paper, that I can use the simpler language.
might be evaluated by calling on their root node the method `Exp[T].expResult(): T`, which returns the result of the expression. Its default implementation simply delegates to the abstract method `private[ivm] def Exp[T].interpret(): T`, which evaluates the expression node it is invoked on; each concrete expression node must provide an implementation. However, `Exp[T].expResult(): T` might also be overridden, for instance to cache its result or maintain it incrementally, while `interpret()` always reevaluates the content. //XXX: not true for Queryable, fix this.
The recursion is done by calling `expResult()` to reuse any cached results. //XXX: fix this too.
Interpretation is in general side-effect-free, with a few exceptions related to incremental view maintenance.
Environments is represented as a `HashMap[Int, Any]`; during interpretation, the topmost environment is stored inside a thread-local global variable and not passed through interpreters.
When evaluating the body of a closure, we save the current environment on the metalanguage stack, add a new binding to the environment saved in the closure and use the result to replace the current environment, evaluate the body and then 

~~~
class FuncExpInt[S, T](val foasBody: Exp[T], v: TypedVar[S]) extends FuncExp[S, T](…) {
  override def interpret(): S => T = {
    //Close over the current environment, and ensure it is stored in the returned closure.
    val env = FuncExpInt.env.get()
    z => FuncExpInt.env.withValue(env + (v.id -> z))(foasBody.interpret())
  }
}

class ScalaThreadLocal[T](v: => T) extends java.lang.ThreadLocal[T] {
  //...
  def withValue[U](tempV: T)(toCompute: => U) = {
    val old = get()

    set(tempV)
    val res = toCompute
    set(old)
    res
  }
}
~~~

<Explanation> Thus, `ScalaThreadLocal.withValue` reuses the host-language stack as the stack of our interpreter.
Open terms might not be evaluated; consequently, if `x: Var` then `x.interpret()` fails, unless a binding for `x` exists in the current environment.

# Incremental view maintenance
## Architecture
Our model for incremental view maintenance (IVM) support is based on the Observer pattern (TODO maybe reference?). The Scala collection library defines a reusable framework to support this pattern, but we use a variant of it. The two main roles are fulfilled by traits Publisher and Subscriber.
Definitions follow [are provided in Fig. x]:

~~~
trait Publisher[+Evt, +Pub <: Publisher[Evt, Pub]] {
  type Sub = Subscriber[Evt, Pub]

  def addSubscriber(sub: Sub)
  def removeSubscriber(sub: Sub)
  protected[this] def publish(evt: Evt)
}

trait MsgSeqPublisher[+T, +Pub <: MsgSeqPublisher[T, Pub]]
  extends Publisher[Seq[Message[T]], Pub] //Simplified
~~~

An expression node of type `Exp[T]` publishes to its subscribers sequences of messages of type `Message[T]`:

`Exp[T] <: MsgSeqPublisher[T, Exp[T]]`

Composite expression nodes subscribe on their children expression nodes. [Is this what we implement? It's not so simple!]

A subscriber receiving a sequence of messages can handle each of them separately, and we provide infrastructure for that; to allow more efficient handling of updates, a subscriber can however also analyze the complete sequence before deciding the update strategy. For instance, if the sequence of updates is long, it might be simpler to recompute the result from scratch.

Many expression nodes handle messages by producing, for each message, a sequence of messages; the generic code then produces a bigger batch by concatenating the produced sequences:

~~~
trait EvtTransformerBase[-T, +U, -Repr] extends MsgSeqSubscriber[T, Repr]
  with MsgSeqPublisher[U, EvtTransformerBase[T, U, Repr]]
{
  //Contract: transforms messages, potentially executes them.
  def transformedMessages(pub: Repr, v: Message[T]): Seq[Message[U]]

  override def notify(pub: Repr, evts: Seq[Message[T]]) {
    publish(evts flatMap (transformedMessages(pub, _)))
  }
}

~~~

Various implementation of Message[T] exist. If T is atomic, it is appropriate to send a message which simply contains the old and new value:

`case class UpdateVal[T](oldV: T, newV: T) extends Message[T]`

However, this message forces subscribers to reexecute the computations they represent. When `T` is a structured type, values of type `Message[T]` can indicate more precisely which part of it changed, to allow listeners to execute only the associated part of the computation. Users can use this framework to observe result of queries and provide updates. We want them (TODO) to be able to provide datatype-specific messages using our framework.

Our library deals in particular with the specific case of collections, and provides specific messages for modifications on Traversables.
We also supply reusable traits for collections publishing their modifications, together with concrete collections using those traits.

## `FlatMap` operations
Incremental maintenance for `FlatMap(collection, function)` nodes requires applying `function` to each element of `collection`, and listening on both `collection` and the results.
However, that applies also to MapOp, only we just didn't implement it yet.

## Recompute messages
When an expression node is incapable of updating its result incrementally, it can also recompute its value and propagate it with a `Recompute` message - which just provides the new value. (XXX: does it?)

### GC
Nodes hold onto their observers through weak references and on nodes they depend on through strong references.

# Related work
## Query embeddings
\textsc{Linq} also represents a deep embedding of queries, even for queries
An embedding of Scala within itself is Lightweight modular staging and language virtualization.
Techniques for embedding operations on Scala collections are also used for e.g. Scalaquery and Scala Integrated Query (XXX citations), a Scala DSEL for queries to SQL databases expressed using for comprehensions and additional operators. Their data model however only considers one type of collections, represented by the interface Iterable, because different types are not necessary for queries on SQL databases (relational algebra does not support different ways to group records, only different records). Our language embedding looks more suitable for CoSQL databases and allows expressing XXX.

Ferry is also a functional language for data queries, but it is implemented by translation to SQL; moreover, they do not have a concept of first-class dictionaries or indexes - XXX develop a stronger comparison.

## Query languages for hierarchical data
\citet{Colby90}, in "A Recursive Algebra for Nested Relations", defines a data-model similar to ours, because it is also compositional.

Various techniques can be seen as allowing representing dictionaries. For instance, a JSON object is a dictionary; since JSON and XML are "similar", one can argue that even XML allows representing dictionaries, especially when those techniques are used in a semi-structural context where no fixed schema is imposed. However, indexes are in a sense _reverse_ maps, and thus are different.

In a relational database, a relation is actually also a dictionary, mapping a value of the primary key to the whole record; however, the language does not explicitly allow using a relation as a dictionary; moreover, there is no uniform way to refer to both relations and indexes.

\citet{Henglein10}, in "Generic Multiset Programming for Language-integrated Querying", present an alternative framework which allows implementing some of the same optimizations we present. In their development however the users are supposed to write the functions used for queries explicitly using operators exposing their structure. In our case, instead, we emulate expression trees to allow the same optimization to be still possible with less involvement of the user. Their model does not reify the queries, preventing their optimization to happen before execution time - this is sensible for the limited optimizations they implement, but not in general. Lazy products, on the other hand, are indeed an interesting idea which we do not support (XXX: should we?).

## Index organizations
\citet{Lee98} describes other indexes. We do not implement them, but they could be represented by our interface. Moreover the path dictionary is tuned to reduce disk I/O, not for memory operations; it is not clear if it could be advantageous because of its reduced cache footprint (we have no clue and don't really care, at least yet).

## Incremental View Maintenance
\citet{Willis08}, in "Caching and Incrementalization in the Java language", do not support reduce and requires a special-purpose compiler,  with the associated complexity of implementation and integration with other extensions (cite our LDTA 2012 paper).

## Incremental computation
\citet{Hammer11} design a semantics for self-adjusting computation for low-level languages. Their work allow supporting a large class of imperative programs; they trace program execution while the initial result is being computed, producing a tree of intermediate results, and can reevaluate this trace when the input is changed. The cost of reevaluation in the worst case is proportional to the height of the computation tree; thus, to support efficient incremental updates, the computation needs to be tree-structured. For instance, updating the result of a linear fold takes linear time if the first element is changed, while a tree-fold takes logarithmic time (as for us). Thus, the example code they present to compute the maximum of a vector implements an in-place destructive tree-fold, which is a non-idiomatic implementation in C; moreover, the programmer needs to control explicitly memoization points to reuse computations. In other words, their library supports incremental maintenance for arbitrary client programs, but does not guarantee efficient performance unless the program is engineered specifically to take advantage of their library.
Our library, instead, does not support arbitrary input programs, but provides an extensible repository of high-level operations for which we studied incremental maintenance algorithms. The implementor of such operations needs to reason about incremental maintenance, but the user is freed from such responsibility. On the other hand, we provide no support for memoization.

\citet{Demetrescu11} provides a different model; objects can be located in a separate store of reactive memory, and developers specify one-way, dataflow constraints in the form of arbitrary groups of imperative statements, which typically transform input data in reactive memory into output data. Modifications in reactive memory are detected, and affected dataflow constraints are re-executed as needed. This model is more explicit than the one by \citet{Hammer11}, but similarly requires the end developer to explicitly structure reactive computation. \citeauthor{Demetrescu11} do not describe reusable building blocks for incremental computation, but they describe the implementation of the Observer pattern, so we believe that an architecture similar to ours could in principle be implemented on top of their platform.

\citet{Burckhardt11} discuss yet another model. (XXX read).

\citet{Ramalingam93} provide an extensive bibliography of previous approaches to incremental computation.

# Future work
It could be nice to integrate this with some (transparent) support for persistence (orthogonal persistence?), like Hibernate, to get an embedded database library. If a remoting library could then support remote clients, we would then have a complete DBMS. However, numerous research challenges exist for something like this.

# References
\bibliography{/Users/pgiarrusso/Documents/Research/PS-Repo/Paolo/DB}
